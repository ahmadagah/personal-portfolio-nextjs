---
title: 'Document Querying'
date: '2024-12-20'
author: 'Ahmad Agah'
---

# Document Querying

We can now test the entire system by setting up a Retrieval Augmented Generation (RAG) chain. This chain takes the user query, retrieves content from the vector database most similar to the query, and sends it to the LLM. The prompt used instructs the model to return source URLs to ensure content is being retrieved from the correct source.

```python
prompt_template = """You are an assistant for question-answering
tasks. Use the following context to answer the question. Provide the
source URLs of the context you used to perform the task and instruct
the user to visit them for more information. If you don't know the
answer, just say that you don't know.

Question: {question}

Context: {context}

Answer: """

prompt = PromptTemplate(
    input_variables=["question"],
    template=prompt_template
)

```

The program includes a function to format the retrieved documents by adding the source URLs to the document content:

```python
def format_docs(docs):
    output = "\n\n".join(doc.page_content for doc in docs)
    sources = {doc.metadata['source'] for doc in docs}
    source_list = "\nSource: ".join(source for source in sources)
    return output + source_list
```

Finally, the RAG chain is created to process the user query and produce the final output:

```python
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)
```
