---
title: 'LLMs and Retrieval Augmented Generation'
date: '2024-12-20'
author: 'Ahmad Agah'
---

# LLMs and Retrieval Augmented Generation

One of the more common LLM applications is a "retrieval augmented generation" (RAG) one which leverages in-context learning to allow an LLM to answer questions based on up-to-date information retrieved after the model has been trained. Such an approach solves the problem of generative models producing or fabricating results that are incorrect, sometimes referred to as hallucinations.

Consider the application below where a user wants to ask a question to a set of documents in a knowledge base. With RAG, one indexes the documents and the question that is being asked by the user. Based on the similarity of questions and particular documents in the index, the question and the relevant document is sent to the generation model to produce the answer.

![RAG Application Example](/images/LLMs-RAGs-workflow.png)
